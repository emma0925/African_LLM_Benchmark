## Welcome to the African_LLM_Benchmark repository! 
This repository contains code and results related to my contributions to the IrokoBench project, a benchmark dataset designed to evaluate Large Language Models (LLMs) in African languages. IrokoBench is part of a broader effort to address the underrepresentation of African languages in NLP research by providing comprehensive benchmarks for 16 low-resource African languages across three tasks: Natural Language Inference (AfriXNLI), Mathematical Reasoning (AfriMGSM), and Multi-choice Knowledge-based QA (AfriMMLU). The paper is currently under review by NeurIPS 2024. We have received 3 out of 4 reviews so far, with scores of 6, 6, and 5.
## My Contributions
In this repository, I have included all the code and results I contributed to the IrokoBench project. My contributions focus on:

Scripts for Auto-Generation of Outputs: I developed scripts for generating outputs for the AfriMMLU, AfriMGSM, and AfriXNLI tasks. These scripts automate the evaluation process across different LLMs and settings (zero-shot, few-shot, and translate-test).

Scoring Functions: I implemented functions to calculate evaluation metrics, including F1, F1-square, and accuracy, for the generated outputs. These metrics are crucial for assessing the performance of LLMs on African languages.
